{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to generate prediction model using features available in FHIR server\n",
    "---------------------------------------------------------------------------------\n",
    "<br>\n",
    "Item IDs used for prediction - 220052, 22045, 224690, 223761, 220277,  227013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyon/Tools/anaconda3/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (8,10,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "def load_data(path2, path4):\n",
    "    #df_icu_stays = pd.read_csv(path1)\n",
    "    df_sepsis_occurance = pd.read_csv(path2)\n",
    "    #df_events = pd.read_csv(path3)\n",
    "    df_chart_events = pd.read_csv(path4)\n",
    "    return df_sepsis_occurance, df_chart_events\n",
    "\n",
    "path1 = 'm100s2/ICUSTAYS.csv'\n",
    "path2 = 'm100s2/sample_ids_2.csv'\n",
    "path3 = 'm100s2/sample_inputevents_mv_2.csv'\n",
    "path4 = \"m100s2/sample_chartevents_2.csv\"\n",
    "df_sepsis_occurance, df_chart_events = load_data(path2, path4)\n",
    "#df_icu_stays, df_sepsis_occurance, df_events, df_chart_events = load_data(path1,path2,path3,path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((100, 4), (3168992, 15))\n"
     ]
    }
   ],
   "source": [
    "print(df_sepsis_occurance.shape, df_chart_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['row_id' 'subject_id' 'hadm_id' 'icustay_id' 'itemid' 'charttime'\n",
      " 'storetime' 'cgid' 'value' 'valuenum' 'valueuom' 'warning' 'error'\n",
      " 'resultstatus' 'stopped']\n"
     ]
    }
   ],
   "source": [
    "print(df_chart_events.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_data(df_sepsis_occurance, df_chart_events): \n",
    "    df1 = pd.merge(df_chart_events, df_sepsis_occurance, on='hadm_id', how='inner')\n",
    "    #df1 = pd.merge(df, df_chart_events, on='hadm_id', how='inner')\n",
    "    return df1\n",
    "\n",
    "df_joined = join_data(df_sepsis_occurance, df_chart_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['row_id' 'subject_id_x' 'hadm_id' 'icustay_id_x' 'itemid' 'charttime'\n",
      " 'storetime' 'cgid' 'value' 'valuenum' 'valueuom' 'warning' 'error'\n",
      " 'resultstatus' 'stopped' 'subject_id_y' 'icustay_id_y' 'sepsist0']\n"
     ]
    }
   ],
   "source": [
    "print(df_joined.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37680\n",
      "0\n",
      "44166\n",
      "11769\n",
      "82788\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_vals = df_joined['itemid'].values\n",
    "ids = [220052, 22045, 224690, 223761, 220277,  227013]\n",
    "for i in ids:\n",
    "    print(len(all_vals[all_vals == i]))\n",
    "[220052, 224690, 223761, 220277]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3372162, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_joined = df_joined[df_joined['itemid'].isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176403, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['row_id', 'subject_id_x', 'hadm_id', 'icustay_id_x', 'itemid',\n",
       "       'charttime', 'storetime', 'cgid', 'value', 'valuenum', 'valueuom',\n",
       "       'warning', 'error', 'resultstatus', 'stopped', 'subject_id_y',\n",
       "       'icustay_id_y', 'sepsist0'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s1 = df_joined[(df_joined.hadm_id == 109151) & (df_joined.itemid == 220277)]\n",
    "df_joined.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s1.columns.values\n",
    "#s1 = pd.Series(s1.charttime)\n",
    "#s1.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4518    2141-06-08 20:30:00\n",
       "4524    2141-06-08 21:00:00\n",
       "4540    2141-06-08 22:00:00\n",
       "4548    2141-06-08 23:00:00\n",
       "4779    2141-06-09 00:00:00\n",
       "6268    2141-06-09 01:00:00\n",
       "6275    2141-06-09 02:00:00\n",
       "6281    2141-06-09 03:00:00\n",
       "6316    2141-06-09 04:00:00\n",
       "8066    2141-06-09 05:00:00\n",
       "8072    2141-06-09 06:00:00\n",
       "8078    2141-06-09 07:00:00\n",
       "8084    2141-06-09 08:00:00\n",
       "8097    2141-06-09 09:00:00\n",
       "8106    2141-06-09 10:00:00\n",
       "8112    2141-06-09 11:00:00\n",
       "3818    2141-06-09 12:47:00\n",
       "3826    2141-06-09 13:00:00\n",
       "4233    2141-06-09 14:00:00\n",
       "4239    2141-06-09 15:00:00\n",
       "4245    2141-06-09 16:00:00\n",
       "5435    2141-06-09 17:00:00\n",
       "5452    2141-06-09 18:00:00\n",
       "5458    2141-06-09 19:00:00\n",
       "5464    2141-06-09 20:00:00\n",
       "7416    2141-06-09 22:30:00\n",
       "7418    2141-06-09 23:00:00\n",
       "7421    2141-06-10 00:00:00\n",
       "3379    2141-06-10 01:00:00\n",
       "3385    2141-06-10 02:00:00\n",
       "               ...         \n",
       "6059    2141-06-27 10:07:00\n",
       "6064    2141-06-27 11:00:00\n",
       "6067    2141-06-27 12:00:00\n",
       "6081    2141-06-27 13:00:00\n",
       "6086    2141-06-27 14:00:00\n",
       "3599    2141-06-27 15:00:00\n",
       "3604    2141-06-27 16:00:00\n",
       "3618    2141-06-27 17:00:00\n",
       "4037    2141-06-27 18:00:00\n",
       "4042    2141-06-27 19:00:00\n",
       "4044    2141-06-27 20:00:00\n",
       "4061    2141-06-27 21:01:00\n",
       "7157    2141-06-27 22:00:00\n",
       "7162    2141-06-27 23:00:00\n",
       "7166    2141-06-28 00:00:00\n",
       "7183    2141-06-28 01:00:00\n",
       "7188    2141-06-28 02:00:00\n",
       "7190    2141-06-28 03:00:00\n",
       "5990    2141-06-28 04:00:00\n",
       "6004    2141-06-28 05:00:00\n",
       "6007    2141-06-28 06:00:00\n",
       "6012    2141-06-28 07:00:00\n",
       "6017    2141-06-28 08:00:00\n",
       "6031    2141-06-28 09:00:00\n",
       "6037    2141-06-28 10:00:00\n",
       "4002    2141-06-28 11:00:00\n",
       "4008    2141-06-28 12:00:00\n",
       "4022    2141-06-28 13:00:00\n",
       "4027    2141-06-28 14:00:00\n",
       "4032    2141-06-28 15:00:00\n",
       "Name: charttime, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert columns to datetime objects \n",
    "df_joined['sepsist0'] = df_joined['sepsist0'].apply(lambda i: datetime.strptime(i,'%Y-%m-%d %H:%M:%S'))\n",
    "#df_joined['starttime'] = df_joined['starttime'].apply(lambda i: datetime.strptime(i,'%Y-%m-%d %H:%M:%S'))\n",
    "df_joined['charttime'] = df_joined['charttime'].apply(lambda i: datetime.strptime(i,'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create time difference in hours column between sepsis t0 and starttime\n",
    "df_joined['time_diff'] = (df_joined['sepsist0'] - df_joined['charttime'])\n",
    "df_joined['time_diff'] = df_joined['time_diff'].apply(lambda i: i.seconds/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id                      3846287\n",
      "subject_id_x                  29030\n",
      "hadm_id                      109151\n",
      "icustay_id_x                 272418\n",
      "itemid                       220052\n",
      "charttime       2141-06-16 15:00:00\n",
      "storetime       2141-06-16 15:34:00\n",
      "cgid                          15047\n",
      "value                            68\n",
      "valuenum                         68\n",
      "valueuom                       mmHg\n",
      "warning                           0\n",
      "error                             0\n",
      "resultstatus                    NaN\n",
      "stopped                         NaN\n",
      "subject_id_y                  29030\n",
      "icustay_id_y                 272418\n",
      "sepsist0        2141-06-08 00:00:00\n",
      "time_diff                         9\n",
      "Name: 1272, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_joined.iloc[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hours = 5\n",
    "#1 if developed sepsis with 8 hours else 0\n",
    "df_joined['curr_sepsis'] = df_joined['time_diff'].apply(lambda i: 1 if i <= hours else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44845\n",
      "131558\n"
     ]
    }
   ],
   "source": [
    "#Number of events with sepsis and without sepsis\n",
    "print(len(df_joined['curr_sepsis'][df_joined['curr_sepsis'] == 1].values))\n",
    "print(len(df_joined['curr_sepsis'][df_joined['curr_sepsis'] == 0].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we select our features and drop NaNs\n",
    "def filter_features(df,features,pred_label):\n",
    "    all_feats = features+pred_label\n",
    "    new_df = df[all_feats]\n",
    "    new_df = new_df.dropna(axis=0)\n",
    "    new_x = new_df[features]\n",
    "    new_y = new_df[pred_label]\n",
    "    return new_x, new_y \n",
    "\n",
    "#features = ['amount','rate','patientweight','totalamount','originalamount','originalrate']\n",
    "features = ['value']\n",
    "pred_label = ['curr_sepsis']\n",
    "filtered_x, filtered_y = filter_features(df_joined,features,pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h4>Rate Feature</h4></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176403\n"
     ]
    }
   ],
   "source": [
    "#Creating the sparse rate feature vectors\n",
    "all_items = df_joined['itemid'].values\n",
    "#all_rates = df_joined['rate'].values\n",
    "all_rates = df_joined['value'].values\n",
    "#all_rates.shape\n",
    "print(all_items.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176403,)\n"
     ]
    }
   ],
   "source": [
    "#Creating the sparse rate feature vectors\n",
    "all_items = df_joined['itemid'].values\n",
    "#all_rates = df_joined['rate'].values\n",
    "all_rates = df_joined['value'].values\n",
    "all_y = list(df_joined['curr_sepsis'])\n",
    "item_ids = list(set(df_joined['itemid'].values)) \n",
    "remap_items = range(len(item_ids))\n",
    "dict_items = dict(zip(item_ids,remap_items))\n",
    "#print(dict_items)\n",
    "item_ids = list(set(df_joined['itemid'].values)) \n",
    "item_id_feature = np.zeros((df_joined.shape[0],len(item_ids)))\n",
    "x = []\n",
    "y = []\n",
    "#print(all_y.shape)\n",
    "#print(all_y[0])\n",
    "for i in range(all_items.shape[0]):\n",
    "    val = all_items[i]\n",
    "    rate = all_rates[i]\n",
    "    remap_val = dict_items[val]\n",
    "    z = np.zeros((1,len(item_ids)))[0]\n",
    "    z[remap_val] = rate\n",
    "    x.append(z)\n",
    "    y.append(all_y[i])\n",
    "    \n",
    "rate_feat_x = np.array(x)\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "# for i in range(all_items.shape[0]): \n",
    "#     val = all_items[i]\n",
    "#     rate = all_rates[i]\n",
    "#     #print(val)\n",
    "#     remap_val = dict_items[val]\n",
    "#     z = np.zeros((1,len(item_ids)))[0]\n",
    "#     z[remap_val] = rate\n",
    "#     x.append(z)\n",
    "#     y.append(all_y[i])\n",
    "# \n",
    "# \n",
    "# print(y.shape)\n",
    "\n",
    "#Removing rows with NaNs\n",
    "nan_index = np.argwhere(np.isnan(rate_feat_x))\n",
    "nan_rows = []\n",
    "for i in nan_index: \n",
    "    nan_rows.append(i[0])\n",
    "rate_feat_x = np.delete(rate_feat_x,nan_rows,0)\n",
    "rate_feat_y = np.delete(y,nan_rows,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyon/Tools/anaconda3/envs/tensorflow/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Create train/test split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#xtrain, xtest, ytrain, ytest = train_test_split(filtered_x, filtered_y, test_size=0.20)\n",
    "\n",
    "#Train/Test split for rate sparse feature vector\n",
    "Rxtrain, Rxtest, Rytrain, Rytest = train_test_split(rate_feat_x, rate_feat_y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.    0.   72.    0.]\n",
      " [   0.    0.    0.  100.]\n",
      " [   0.   24.    0.    0.]\n",
      " ..., \n",
      " [   0.    0.    0.   96.]\n",
      " [   0.    0.    0.  100.]\n",
      " [   0.    0.    0.   99.]]\n"
     ]
    }
   ],
   "source": [
    "print(Rxtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h4>Building A Random Forest Model</h4></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyon/Tools/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "#Create Random Forest Model \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators= 25, max_depth= None,random_state= 11 )\n",
    "rf.fit(xtrain, ytrain)\n",
    "prediction = rf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.223406366033\n"
     ]
    }
   ],
   "source": [
    "#Mean Squared Error of Random Forest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(ytest,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776593633967\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Random Forest\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(accuracy_score(ytest,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# now you can save it to a file\n",
    "joblib.dump(rf, 'random_forests_CE.pkl') \n",
    "# and later you can load it\n",
    "rf = joblib.load('random_forests_CE.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rate Sparse Feature:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.745642130325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.externals import joblib\n",
    "rf = RandomForestClassifier(n_estimators= 25, max_depth= None,random_state= 11 )\n",
    "rf.fit(Rxtrain, Rytrain)\n",
    "Rprediction = rf.predict(Rxtest)\n",
    "print(accuracy_score(Rytest,Rprediction))\n",
    "# now you can save it to a file\n",
    "joblib.dump(rf, 'random_forests_CE.pkl') \n",
    "# and later you can load it\n",
    "rf = joblib.load('random_forests_CE.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h4>Building A Logistic Regression Model</h4></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Adamlieberman/anaconda/envs/cse6240hw1/lib/python3.5/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression(C=10) \n",
    "lr.fit(xtrain, ytrain)\n",
    "predictions = lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.216813549986\n",
      "0.783186450014\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(ytest,predictions))\n",
    "print(accuracy_score(ytest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# now you can save it to a file\n",
    "joblib.dump(rf, 'logistic_regression.pkl') \n",
    "# and later you can load it\n",
    "lr = joblib.load('logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
